








pip install nltk spacy beautifulsoup4 keras tensorflow keras-nlp scikit-learn torch pandas matplotlib scipy



import os
import numpy as np
import pandas as pd
from collections import Counter
import nltk
import spacy
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from scipy import stats





dataset = pd.read_csv('data.csv', sep='\t')
dataset.head()


# prompt: select only clean columns and show statistics on  the dataset related to NLP

dataset = dataset[['clean_title', 'clean_selftext','id']]
dataset.dropna(inplace=True)
dataset.info()

# Word count statistics
dataset['title_word_count'] = dataset['clean_title'].apply(lambda x: len(str(x).split()))
dataset['comment_word_count'] = dataset['clean_selftext'].apply(lambda x: len(str(x).split()))
print("Title Word Count Statistics:")
print(dataset['title_word_count'].describe())
print("\nComment Word Count Statistics:")
print(dataset['comment_word_count'].describe())



# prompt: performing tokenization, as 'processed_comment'
# lemmatization, normalization(stemming and lemmatization), stop-word removal, and word vectorization. after show the modified dataset statistics
nltk.download('punkt')
# Tokenization
dataset['processed_comment'] = dataset['clean_selftext'].apply(nltk.word_tokenize)

# Normalization (Stemming and Lemmatization) and Stop-word Removal
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(nltk.corpus.stopwords.words('english'))
lemmatizer = nltk.stem.WordNetLemmatizer()

def normalize_text(tokens):
    normalized_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words and word.isalnum()]
    return normalized_tokens

dataset['processed_comment'] = dataset['processed_comment'].apply(normalize_text)

# Word Vectorization (using a simple count vectorizer for demonstration)
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([' '.join(tokens) for tokens in dataset['processed_comment']])

# Modified Dataset Statistics
print("Modified Dataset Statistics:")
print(dataset.head())
print("\nShape of word vector matrix:", X.shape)


sequences = []
for tokens in dataset['processed_comment']:
  for i in range(1, len(tokens)):
    sequence = tokens[:i]
    sequences.append(sequence)

# Tokenize and pad sequences
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(sequences)
sequences = tokenizer.texts_to_sequences(sequences)
vocab_size = len(tokenizer.word_index) + 1
max_length = max([len(seq) for seq in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')

# Split data into input (X) and output (y)
X = padded_sequences[:, :-1]
y = padded_sequences[:, -1]
y = np.array(tf.keras.utils.to_categorical(y, num_classes=vocab_size))

# Build RNN model
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=max_length - 1))
model.add(SimpleRNN(16))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model (adjust epochs and batch_size as needed)
rnn_history = model.fit(X, y, epochs=10, verbose=1)

# Example prediction
test_sentence = "this is a test"
test_sequence = tokenizer.texts_to_sequences([test_sentence])[0]
padded_test_sequence = pad_sequences([test_sequence], maxlen=max_length - 1, padding='pre')
prediction = model.predict(padded_test_sequence, verbose=0)
predicted_word_index = np.argmax(prediction)
predicted_word = tokenizer.index_word[predicted_word_index]
print(f"Predicted next word: {predicted_word}")






loss_rnn, accuracy_rnn = model.evaluate(X, y, verbose=0)
print('RNN Accuracy: %f' % (accuracy * 100))
rnn_predictions = np.argmax(model.predict(X), axis=1)



# Build LSTM model
lstm_model = Sequential()
lstm_model.add(Embedding(vocab_size, 100, input_length=max_length - 1))
lstm_model.add(LSTM(16))
lstm_model.add(Dense(vocab_size, activation='softmax'))

# Compile the model
lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model 
lstm_history = lstm_model.fit(X, y, epochs=10, verbose = 1)

# Evaluate the model
loss_lstm, accuracy_lstm = lstm_model.evaluate(X, y, verbose=0)
print('LSTM Accuracy: %f' % (accuracy * 100))




# --- Performance Comparison ---
# Assuming you've already trained and evaluated both RNN and LSTM models as shown in the preceding code

# 1. Perplexity (Not directly available in Keras, but can be approximated)
#    We'll use cross-entropy loss as a proxy for perplexity. Lower is better.
print("RNN Cross-entropy Loss (proxy for perplexity):", loss_rnn)  # 'loss' from RNN evaluation
print("LSTM Cross-entropy Loss (proxy for perplexity):", loss_lstm)

# 2. Accuracy (Already calculated)
print("RNN Accuracy:", accuracy_rnn * 100)  # 'accuracy' from RNN evaluation
print("LSTM Accuracy:", accuracy_lstm * 100)  # 'accuracy' from LSTM evaluation

# --- Visualizing Loss Curves ---
plt.plot(rnn_history.history['loss'], label='RNN Training Loss')
plt.plot(lstm_history.history['loss'], label='LSTM Training Loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()






