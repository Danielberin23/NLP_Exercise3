{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL0f2SGp2T4E"
   },
   "source": [
    "# TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwEh3AdS2eny"
   },
   "source": [
    "**Configuring Enviorment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8wNu87731OU"
   },
   "source": [
    "installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYCQ9zV71rDr",
    "outputId": "8734c51d-e2e7-451c-b537-bae8383877ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: keras in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: keras-nlp in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: torch in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (68.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: tf-nightly in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from keras-nlp) (2.18.0.dev20240717)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: tf-nightly-intel==2.18.0-dev20240717 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tf-nightly->keras-nlp) (2.18.0.dev20240717)\n",
      "Requirement already satisfied: tb-nightly~=2.18.0.a in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tf-nightly-intel==2.18.0-dev20240717->tf-nightly->keras-nlp) (2.18.0a20240718)\n",
      "Requirement already satisfied: keras-nightly>=3.2.0.dev in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tf-nightly-intel==2.18.0-dev20240717->tf-nightly->keras-nlp) (3.4.1.dev2024071803)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\daniel\\pycharmprojects\\crackingthecodinginterviewquestions\\.venv\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy beautifulsoup4 keras tensorflow keras-nlp scikit-learn torch pandas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "f6u-yy5A30DY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4G7LnhAw6p9"
   },
   "source": [
    "***Load data***\n",
    "\n",
    "this data is not from homework 1 and 2 cause i found a better way to extract data from reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "mI18dOIgy_Xd",
    "outputId": "b311193f-70f0-4610-fa96-973758c03cf4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>id</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_selftext</th>\n",
       "      <th>clean_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm f'ed up and I cannot fix it no matter what...</td>\n",
       "      <td>When I was younger I was fat, but I'm hilariou...</td>\n",
       "      <td>1e4xvti</td>\n",
       "      <td>im fed up and i cannot fix it no matter what i do</td>\n",
       "      <td>when i was younger i was fat but im hilarious ...</td>\n",
       "      <td>exvti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm 45 years of age and still suck my thumb as...</td>\n",
       "      <td>I'm 45 M and still suck my thumb and honestly ...</td>\n",
       "      <td>1e4xvcj</td>\n",
       "      <td>im  years of age and still suck my thumb as a ...</td>\n",
       "      <td>im  m and still suck my thumb and honestly don...</td>\n",
       "      <td>exvcj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I only paid for half of the price of a cake, k...</td>\n",
       "      <td>More than a year ago, I went to my local groce...</td>\n",
       "      <td>1e4iomq</td>\n",
       "      <td>i only paid for half of the price of a cake kn...</td>\n",
       "      <td>more than a year ago i went to my local grocer...</td>\n",
       "      <td>eiomq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I lied on my resume because I was desperate fo...</td>\n",
       "      <td>I recently graduated from grad school and have...</td>\n",
       "      <td>1e4fg9e</td>\n",
       "      <td>i lied on my resume because i was desperate fo...</td>\n",
       "      <td>i recently graduated from grad school and have...</td>\n",
       "      <td>efge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Extreme Remorse] I relieved my pets from suff...</td>\n",
       "      <td>[Extreme Remorse]\\n\\nI grew up with pets. Yeah...</td>\n",
       "      <td>1e4cg3w</td>\n",
       "      <td>extreme remorse i relieved my pets from suffering</td>\n",
       "      <td>extreme remorse\\n\\ni grew up with pets yeah yo...</td>\n",
       "      <td>ecgw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  I'm f'ed up and I cannot fix it no matter what...   \n",
       "1           1  I'm 45 years of age and still suck my thumb as...   \n",
       "2           2  I only paid for half of the price of a cake, k...   \n",
       "3           3  I lied on my resume because I was desperate fo...   \n",
       "4           4  [Extreme Remorse] I relieved my pets from suff...   \n",
       "\n",
       "                                            selftext       id  \\\n",
       "0  When I was younger I was fat, but I'm hilariou...  1e4xvti   \n",
       "1  I'm 45 M and still suck my thumb and honestly ...  1e4xvcj   \n",
       "2  More than a year ago, I went to my local groce...  1e4iomq   \n",
       "3  I recently graduated from grad school and have...  1e4fg9e   \n",
       "4  [Extreme Remorse]\\n\\nI grew up with pets. Yeah...  1e4cg3w   \n",
       "\n",
       "                                         clean_title  \\\n",
       "0  im fed up and i cannot fix it no matter what i do   \n",
       "1  im  years of age and still suck my thumb as a ...   \n",
       "2  i only paid for half of the price of a cake kn...   \n",
       "3  i lied on my resume because i was desperate fo...   \n",
       "4  extreme remorse i relieved my pets from suffering   \n",
       "\n",
       "                                      clean_selftext clean_id  \n",
       "0  when i was younger i was fat but im hilarious ...    exvti  \n",
       "1  im  m and still suck my thumb and honestly don...    exvcj  \n",
       "2  more than a year ago i went to my local grocer...    eiomq  \n",
       "3  i recently graduated from grad school and have...     efge  \n",
       "4  extreme remorse\\n\\ni grew up with pets yeah yo...     ecgw  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data.csv', sep='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pZEbdkmpOvv",
    "outputId": "f75a4637-620c-4069-9b50-113b893df839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 822 entries, 0 to 823\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   clean_title     822 non-null    object\n",
      " 1   clean_selftext  822 non-null    object\n",
      " 2   id              822 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 25.7+ KB\n",
      "Title Word Count Statistics:\n",
      "count    822.000000\n",
      "mean      13.357664\n",
      "std        3.936284\n",
      "min        1.000000\n",
      "25%       11.000000\n",
      "50%       13.000000\n",
      "75%       15.000000\n",
      "max       53.000000\n",
      "Name: title_word_count, dtype: float64\n",
      "\n",
      "Comment Word Count Statistics:\n",
      "count     822.000000\n",
      "mean      257.379562\n",
      "std       243.037507\n",
      "min         3.000000\n",
      "25%       109.000000\n",
      "50%       194.500000\n",
      "75%       326.750000\n",
      "max      3089.000000\n",
      "Name: comment_word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# prompt: select only clean columns and show statistics on  the dataset related to NLP\n",
    "\n",
    "dataset = dataset[['clean_title', 'clean_selftext','id']]\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.info()\n",
    "\n",
    "# Word count statistics\n",
    "dataset['title_word_count'] = dataset['clean_title'].apply(lambda x: len(str(x).split()))\n",
    "dataset['comment_word_count'] = dataset['clean_selftext'].apply(lambda x: len(str(x).split()))\n",
    "print(\"Title Word Count Statistics:\")\n",
    "print(dataset['title_word_count'].describe())\n",
    "print(\"\\nComment Word Count Statistics:\")\n",
    "print(dataset['comment_word_count'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKsVW-Hzx-a3",
    "outputId": "80dd7836-a82b-4ce8-9096-faca6cef7031"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Dataset Statistics:\n",
      "                                         clean_title  \\\n",
      "0  im fed up and i cannot fix it no matter what i do   \n",
      "1  im  years of age and still suck my thumb as a ...   \n",
      "2  i only paid for half of the price of a cake kn...   \n",
      "3  i lied on my resume because i was desperate fo...   \n",
      "4  extreme remorse i relieved my pets from suffering   \n",
      "\n",
      "                                      clean_selftext       id  \\\n",
      "0  when i was younger i was fat but im hilarious ...  1e4xvti   \n",
      "1  im  m and still suck my thumb and honestly don...  1e4xvcj   \n",
      "2  more than a year ago i went to my local grocer...  1e4iomq   \n",
      "3  i recently graduated from grad school and have...  1e4fg9e   \n",
      "4  extreme remorse\\n\\ni grew up with pets yeah yo...  1e4cg3w   \n",
      "\n",
      "   title_word_count  comment_word_count  \\\n",
      "0                13                  99   \n",
      "1                14                  99   \n",
      "2                12                 306   \n",
      "3                11                 192   \n",
      "4                 8                 744   \n",
      "\n",
      "                                   processed_comment  \n",
      "0  [younger, fat, im, hilarious, considered, funn...  \n",
      "1  [im, still, suck, thumb, honestly, dont, want,...  \n",
      "2  [year, ago, went, local, grocery, store, costu...  \n",
      "3  [recently, graduated, grad, school, struggling...  \n",
      "4  [extreme, remorse, grew, pet, yeah, probably, ...  \n",
      "\n",
      "Shape of word vector matrix: (822, 9380)\n"
     ]
    }
   ],
   "source": [
    "# prompt: performing tokenization, as 'processed_comment'\n",
    "# lemmatization, normalization(stemming and lemmatization), stop-word removal, and word vectorization. after show the modified dataset statistics\n",
    "nltk.download('punkt')\n",
    "# Tokenization\n",
    "dataset['processed_comment'] = dataset['clean_selftext'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Normalization (Stemming and Lemmatization) and Stop-word Removal\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    normalized_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words and word.isalnum()]\n",
    "    return normalized_tokens\n",
    "\n",
    "dataset['processed_comment'] = dataset['processed_comment'].apply(normalize_text)\n",
    "\n",
    "# Word Vectorization (using a simple count vectorizer for demonstration)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([' '.join(tokens) for tokens in dataset['processed_comment']])\n",
    "\n",
    "# Modified Dataset Statistics\n",
    "print(\"Modified Dataset Statistics:\")\n",
    "print(dataset.head())\n",
    "print(\"\\nShape of word vector matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "OUa_o9WvW_mY",
    "outputId": "cee19d59-7cdb-4ecd-9a67-b3a8a34859c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel\\PycharmProjects\\CrackingtheCodingInterviewquestions\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 142ms/step - accuracy: 0.0152 - loss: 6.8070\n",
      "Epoch 2/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 148ms/step - accuracy: 0.0178 - loss: 6.2904\n",
      "Epoch 3/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 146ms/step - accuracy: 0.0253 - loss: 6.2212\n",
      "Epoch 4/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 146ms/step - accuracy: 0.0322 - loss: 6.1329\n",
      "Epoch 5/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 147ms/step - accuracy: 0.0425 - loss: 6.0249\n",
      "Epoch 6/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 149ms/step - accuracy: 0.0519 - loss: 5.9232\n",
      "Epoch 7/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 150ms/step - accuracy: 0.0572 - loss: 5.8512\n",
      "Epoch 8/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 151ms/step - accuracy: 0.0604 - loss: 5.8008\n",
      "Epoch 9/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 151ms/step - accuracy: 0.0611 - loss: 5.7530\n",
      "Epoch 10/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 146ms/step - accuracy: 0.0627 - loss: 5.7185\n",
      "Predicted next word: year\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "for tokens in dataset['processed_comment']:\n",
    "  for i in range(1, len(tokens)):\n",
    "    sequence = tokens[:i]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Split data into input (X) and output (y)\n",
    "X = padded_sequences[:, :-1]\n",
    "y = padded_sequences[:, -1]\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=vocab_size))\n",
    "\n",
    "# Build RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_length - 1))\n",
    "model.add(SimpleRNN(16))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (adjust epochs and batch_size as needed)\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "# Example prediction\n",
    "test_sentence = \"this is a test\"\n",
    "test_sequence = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "padded_test_sequence = pad_sequences([test_sequence], maxlen=max_length - 1, padding='pre')\n",
    "prediction = model.predict(padded_test_sequence, verbose=0)\n",
    "predicted_word_index = np.argmax(prediction)\n",
    "predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "print(f\"Predicted next word: {predicted_word}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wwSolMnDakzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2814075e-01, 3.4738597e-02, 9.4164386e-03, ..., 1.9855975e-10,\n",
       "        2.0360254e-10, 2.0174816e-10],\n",
       "       [1.2814075e-01, 3.4738597e-02, 9.4164386e-03, ..., 1.9855975e-10,\n",
       "        2.0360254e-10, 2.0174816e-10],\n",
       "       [1.2814075e-01, 3.4738597e-02, 9.4164386e-03, ..., 1.9855975e-10,\n",
       "        2.0360254e-10, 2.0174816e-10]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example from class to test\n",
    "test_data = [\n",
    "    \"I hate programming\",\n",
    "    \"NLP is boring\",\n",
    "    \"Recurrent neural networks are cool\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "model.predict(test_padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJS5cLlKjHb_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 186ms/step - accuracy: 0.0150 - loss: 6.8163\n",
      "Epoch 2/10\n",
      "\u001b[1m3068/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 187ms/step - accuracy: 0.0162 - loss: 6.3051\n",
      "Epoch 3/10\n",
      "\u001b[1m2026/3068\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 195ms/step - accuracy: 0.0204 - loss: 6.2613"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(vocab_size, 100, input_length=max_length - 1))\n",
    "lstm_model.add(LSTM(16))\n",
    "lstm_model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "lstm_model.fit(X, y, epochs=10, verbose = 1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = lstm_model.evaluate(X, y, verbose=0)\n",
    "print('LSTM Accuracy: %f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
